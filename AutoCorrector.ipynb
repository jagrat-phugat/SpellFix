{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d7d83a",
   "metadata": {},
   "source": [
    "# ðŸ”¤ Autocorrector Feature Using NLP in Python\n",
    "This Jupyter Notebook demonstrates how to implement an autocorrector using Natural Language Processing (NLP).\n",
    "\n",
    "## ðŸ“Œ Overview\n",
    "The notebook follows these key steps:\n",
    "- **Data Preprocessing**: Tokenization, text cleaning, and frequency analysis.\n",
    "- **Error Correction**: Implementing spelling correction techniques.\n",
    "- **Evaluation**: Testing and improving the model.\n",
    "\n",
    "## ðŸ›  Requirements\n",
    "- Python 3\n",
    "- NLTK / TextBlob / SymSpell (any suitable NLP library)\n",
    "- Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5803bd9",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Importing Required Libraries\n",
    "Let's start by importing the necessary libraries for NLP processing and autocorrection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWPEElC5gdNE",
    "outputId": "4acff889-7378-43db-f828-728fbce7f957"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2534a",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Preprocessing\n",
    "In this section, we clean and preprocess the text data. This includes:\n",
    "- Removing unnecessary symbols and punctuations\n",
    "- Converting text to lowercase\n",
    "- Tokenization and frequency analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S9DFq24sPICo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Jagrat Phugat\\AppData\\Local\\Temp\\ipykernel_7284\\1659752497.py:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  w = re.findall('\\w+', file_name_data)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "w = []\n",
    "\n",
    "with open('final.txt', 'r', encoding=\"utf8\") as f:\n",
    "    file_name_data = f.read()\n",
    "    file_name_data = file_name_data.lower()\n",
    "    w = re.findall('\\w+', file_name_data)\n",
    "\n",
    "main_set = set(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d14140",
   "metadata": {},
   "source": [
    "## ðŸ” Implementing the Autocorrection Algorithm\n",
    "We use NLP techniques to detect and correct misspelled words. Possible approaches include:\n",
    "- **Edit Distance** (Levenshtein distance)\n",
    "- **Probability-based corrections** (using word frequencies)\n",
    "- **Pre-trained models** for contextual spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EwJ70XY5PLRU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def counting_words(words):\n",
    "\tword_count = {}\n",
    "\tfor word in words:\n",
    "\t\tif word in word_count:\n",
    "\t\t\tword_count[word] += 1\n",
    "\t\telse:\n",
    "\t\t\tword_count[word] = 1\n",
    "\treturn word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dea899",
   "metadata": {},
   "source": [
    "## ðŸ† Evaluating the Model\n",
    "We test the autocorrector with sample sentences and analyze its accuracy. Further improvements can be made using deep learning-based language models like BERT or GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IPSBoHg7PMue"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prob_cal(word_count_dict):\n",
    "\tprobs = {}\n",
    "\tm = sum(word_count_dict.values())\n",
    "\tfor key in word_count_dict.keys():\n",
    "\t\tprobs[key] = word_count_dict[key] / m\n",
    "\treturn probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oWWj0paFPT5x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jagrat\n",
      "[nltk_data]     Phugat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jagrat\n",
      "[nltk_data]     Phugat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def LemmWord(word):\n",
    "    return [lemmatizer.lemmatize(wd) for wd in word.split()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PYNIYRD1PVSa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def DeleteLetter(word):\n",
    "\tdelete_list = []\n",
    "\tsplit_list = []\n",
    "\n",
    "\tfor i in range(len(word)):\n",
    "\t\tsplit_list.append((word[0:i], word[i:]))\n",
    "\n",
    "\tfor a, b in split_list:\n",
    "\t\tdelete_list.append(a + b[1:])\n",
    "\treturn delete_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KpPKjSm1PX9O"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Switch_(word):\n",
    "\tsplit_list = []\n",
    "\tswitch_l = []\n",
    "\n",
    "\tfor i in range(len(word)):\n",
    "\t\tsplit_list.append((word[0:i], word[i:]))\n",
    "\n",
    "\tswitch_l = [a + b[1] + b[0] + b[2:] for a, b in split_list if len(b) >= 2]\n",
    "\treturn switch_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qKrmByzdPZRQ"
   },
   "outputs": [],
   "source": [
    "def Replace_(word):\n",
    "\tsplit_l = []\n",
    "\treplace_list = []\n",
    "\n",
    "\tfor i in range(len(word)):\n",
    "\t\tsplit_l.append((word[0:i], word[i:]))\n",
    "\talphs = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\treplace_list = [a + l + (b[1:] if len(b) > 1 else '')\n",
    "\t\t\t\t\tfor a, b in split_l if b for l in alphs]\n",
    "\treturn replace_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pIo2sis8PamC"
   },
   "outputs": [],
   "source": [
    "def insert_(word):\n",
    "\tsplit_l = []\n",
    "\tinsert_list = []\n",
    "\n",
    "\tfor i in range(len(word) + 1):\n",
    "\t\tsplit_l.append((word[0:i], word[i:]))\n",
    "\n",
    "\talphs = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\tinsert_list = [a + l + b for a, b in split_l for l in alphs]\n",
    "\treturn insert_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2GWugWIpPbxa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def colab_1(word, allow_switches=True):\n",
    "\tcolab_1 = set()\n",
    "\tcolab_1.update(DeleteLetter(word))\n",
    "\tif allow_switches:\n",
    "\t\tcolab_1.update(Switch_(word))\n",
    "\tcolab_1.update(Replace_(word))\n",
    "\tcolab_1.update(insert_(word))\n",
    "\treturn colab_1\n",
    "\n",
    "def colab_2(word, allow_switches=True):\n",
    "\tcolab_2 = set()\n",
    "\tedit_one = colab_1(word, allow_switches=allow_switches)\n",
    "\tfor w in edit_one:\n",
    "\t\tif w:\n",
    "\t\t\tedit_two = colab_1(w, allow_switches=allow_switches)\n",
    "\t\t\tcolab_2.update(edit_two)\n",
    "\treturn colab_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "stxdwUS8PdKm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_corrections(word, probs, vocab, n=2):\n",
    "\tsuggested_word = []\n",
    "\tbest_suggestion = []\n",
    "\tsuggested_word = list(\n",
    "\t\t(word in vocab and word) or colab_1(word).intersection(vocab)\n",
    "\t\tor colab_2(word).intersection(\n",
    "\t\t\tvocab))\n",
    "\n",
    "\tbest_suggestion = [[s, probs[s]] for s in list(reversed(suggested_word))]\n",
    "\treturn best_suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoGnLY3qPebW",
    "outputId": "ed37b7c2-a14d-4d4d-c6eb-91438a3364f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marry\n",
      "marrie\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_word = input(\"Enter any Word: \")\n",
    "\n",
    "word_count = counting_words(main_set)\n",
    "\n",
    "def probab_cal(word_count):\n",
    "    total_words = sum(word_count.values())\n",
    "    probs = {word: count/total_words for word, count in word_count.items()}\n",
    "    return probs\n",
    "\n",
    "probs = probab_cal(word_count)\n",
    "\n",
    "tmp_corrections = get_corrections(my_word, probs, main_set, 2)\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "\tif(i < 3):\n",
    "\t\tprint(word_prob[0])\n",
    "\telse:\n",
    "\t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51638423",
   "metadata": {},
   "source": [
    "## ðŸ“ Conclusion & Future Enhancements\n",
    "This project successfully demonstrates a basic NLP-based autocorrector. Future improvements can include:\n",
    "- Enhancing accuracy with larger text corpora\n",
    "- Implementing deep learning models for contextual corrections\n",
    "- Integrating the feature into real-world applications like chatbots"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
